{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330585e3-fb0d-4041-ba90-54b7cc1b80f0",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# This cell is NOT editable. Overwrite variables on your own discretion.\n",
    "# Any changes other than the script code will NOT BE SAVED!\n",
    "# All cells are assumed to be script code cells, unless explictly tagged as 'o9_ignore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2e9ce5-4d43-45b6-8e13-ce6a38751252",
   "metadata": {
    "tags": [
     "o9_ignore"
    ]
   },
   "outputs": [],
   "source": [
    "_factTable = \"Select ([Supplier].[Supplier Location] * [Activity2].[Activity2] * [Location].[Location] * [Documents].[OrderlineID] * [Item].[Item] * [Version].[Version Name] * [Time].[Day] ) on row,  ({Measure.[AL PO % of Total Goods Receipt TG], Measure.[AL PO % of Total Purchase Value TG], Measure.[AL PO % of Total Unique Goods Purchased TG], Measure.[AL PO Commit Creation Date TG], Measure.[AL PO Commit Delivery Date TG], Measure.[AL PO Commit Delivery Qty TG], Measure.[AL PO Goods Receipt Date TG], Measure.[AL PO Goods Receipt Purchase Value TG], Measure.[AL PO Goods Receipt Quantity TG], Measure.[AL PO Header Creation Date TG], Measure.[AL PO Net Price Per Unit TG], Measure.[AL PO Unique Goods Purchased TG]}) on column;\"\n",
    "_clusterItem = \"Select ([Version].[Version Name] * [Item].[Item]) on row,  ({Measure.[Cluster]}) on column;\"\n",
    "\n",
    "\n",
    "# Initialize the O9DataLake with the input parameters and dataframes\n",
    "# Data can be accessed with O9DataLake.get(<Input Name>)\n",
    "# Overwritten values will not be reflected in the O9DataLake after initialization\n",
    "\n",
    "from o9_common_utils.O9DataLake import O9DataLake, ResourceType, DataSource\n",
    "factTable = O9DataLake.register(\"factTable\",DataSource.LS, ResourceType.IBPL, _factTable)\n",
    "clusterItem = O9DataLake.register(\"clusterItem\",DataSource.LS, ResourceType.IBPL, _clusterItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a0c602-7760-4b6a-9017-cf49f93a593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from o9helpers import user_storage_path\n",
    "from o9storage import cloud_storage_utils, storage_utils\n",
    "import pandas as pd\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import math\n",
    "\n",
    "logger = logging.getLogger('o9_logger')\n",
    "logger.info('===== Pickle Script for ActionButton ML Model Pickle =======')\n",
    "\n",
    "\n",
    "versionName = factTable['Version.[Version Name]'].iloc[0]\n",
    "\n",
    "\n",
    "logger.info(\"***** Version Name *******\")\n",
    "logger.info(versionName)\n",
    "\n",
    "factTable = factTable[[\"Supplier.[Supplier Location]\",\"Activity2.[Activity2]\",\n",
    "                       \"Location.[Location]\",\"Item.[Item]\",\"Time.[Day]\",\n",
    "                       \"AL PO Header Creation Date TG\",\n",
    "                       \"AL PO Goods Receipt Quantity TG\",\n",
    "                       \"AL PO Goods Receipt Date TG\",\n",
    "                       \"AL PO Net Price Per Unit TG\",\n",
    "                      ]]\n",
    "\n",
    "factTable = factTable.dropna()\n",
    "\n",
    "factTable['AL PO Goods Receipt Date TG'] = pd.to_datetime(factTable['AL PO Goods Receipt Date TG'])\n",
    "factTable['AL PO Header Creation Date TG'] = pd.to_datetime(factTable['AL PO Header Creation Date TG'])\n",
    "factTable['LeadTime'] = (factTable['AL PO Goods Receipt Date TG'] - factTable['AL PO Header Creation Date TG']).dt.days\n",
    "\n",
    "factTable['year'] = factTable['AL PO Header Creation Date TG'].dt.year\n",
    "factTable['year'] = factTable['year'].astype(int)\n",
    "factTable['year'] = factTable['year'].astype(str)\n",
    "\n",
    "factTable[\"thanksgiving\"] = factTable['year'] + \"-11-26\"\n",
    "factTable[\"easter\"] = factTable['year'] + \"-04-12\"\n",
    "factTable[\"usIDay\"] = factTable['year'] + \"-07-04\"\n",
    "factTable[\"christmasDay\"] = factTable['year'] + \"-12-25\"\n",
    "\n",
    "list_holidays = [\"thanksgiving\",\"easter\",\"usIDay\",\"christmasDay\"]\n",
    "\n",
    "#please write a function for this:\n",
    "for i in list_holidays:\n",
    "    factTable[i] = pd.to_datetime(factTable[i])\n",
    "\n",
    "for i in list_holidays:\n",
    "    factTable[\"date_diff_\" + i] = (factTable[\"AL PO Header Creation Date TG\"] - factTable[i]).dt.days\n",
    "    \n",
    "factTable['MonthInt'] = factTable['AL PO Header Creation Date TG'].dt.month\n",
    "factTable['cos_month'] = factTable['MonthInt'].apply(lambda x: math.cos(2*3.14*x/12))\n",
    "\n",
    "factTable['Supplier.[Supplier]'] = factTable['Supplier.[Supplier Location]']\n",
    "\n",
    "factTable = factTable.merge(clusterItem,on='Item.[Item]')\n",
    "\n",
    "factTable = factTable.drop(['Supplier.[Supplier Location]'],axis=1)\n",
    "\n",
    "factTable['year'] = factTable['AL PO Header Creation Date TG'].dt.year\n",
    "factTable = factTable.drop(['AL PO Header Creation Date TG','AL PO Goods Receipt Date TG'],axis=1)\n",
    "\n",
    "factTable['Time.[Day]'] = pd.to_datetime(factTable['Time.[Day]'])\n",
    "factTable['Month'] = factTable['Time.[Day]'].apply(lambda x: x.strftime(\"%b\"))\n",
    "factTable = factTable.drop([\"Time.[Day]\"],axis=1)\n",
    "\n",
    "#factTable = factTable[factTable[\"year\"]==2019]\n",
    "\n",
    "factTable1 = factTable.copy()\n",
    "logger.info(\"***** FactTable ****\")\n",
    "logger.info(factTable1)\n",
    "\n",
    "factTable1 = factTable1[factTable1.groupby(\"Cluster\").LeadTime.transform(lambda x : (x<x.quantile(0.80))&(x>(x.quantile(0.10)))).eq(1)]\n",
    "\n",
    "factTable1 = factTable1.rename(columns={'AL PO Net Price Per Unit TG':'Price',\n",
    "                                        'AL PO Goods Receipt Quantity TG':'Quantity'})\n",
    "\n",
    "\n",
    "X = factTable1.copy()\n",
    "\n",
    "####################### WITHOUT PRICE MODEL #######################\n",
    "\n",
    "X1 = X[[\"Supplier.[Supplier]\", \n",
    "        \"cos_month\",\n",
    "        \"Cluster\",\n",
    "        \"Activity2.[Activity2]\",\n",
    "        \"Location.[Location]\",\n",
    "        'Quantity',\n",
    "        'date_diff_thanksgiving',\n",
    "        'date_diff_easter',\n",
    "        'date_diff_usIDay',\n",
    "        'date_diff_christmasDay']]\n",
    "\n",
    "y = X[['LeadTime']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X1, y, random_state=42)\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "       ('imputer', SimpleImputer(strategy='mean'))\n",
    "     ])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "       ('imputer', SimpleImputer(strategy='constant'))\n",
    "      ,('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "numeric_features = ['cos_month','Cluster','Quantity','date_diff_thanksgiving',\n",
    "        'date_diff_easter',\n",
    "        'date_diff_usIDay',\n",
    "        'date_diff_christmasDay']\n",
    "\n",
    "categorical_features = [\"Supplier.[Supplier]\",\"Activity2.[Activity2]\",\"Location.[Location]\"]\n",
    "\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "   transformers=[\n",
    "    ('numeric', numeric_transformer, numeric_features)\n",
    "   ,('categorical', categorical_transformer, categorical_features)\n",
    "]) \n",
    "\n",
    "\n",
    "percentileList = [50,75,95]\n",
    "\n",
    "### For ML Model #####\n",
    "\n",
    "for i in percentileList:\n",
    "    logger.info(\"***** Training for Percentile *******\")\n",
    "    logger.info(i)\n",
    "    model = Pipeline([('preprocessor', preprocessor),('classifier', GradientBoostingRegressor(loss=\"quantile\",\n",
    "                                        n_estimators=500, \n",
    "                                        learning_rate=0.1,\n",
    "                                        max_depth=5, \n",
    "                                        random_state=0,\n",
    "                                        alpha=(i/100)))])    \n",
    "    model.fit(X1,y)\n",
    "\n",
    "    bucket = \"MLPipelineModelNOPrice\" + str(i)\n",
    "    local_storage_path = os.path.join(user_storage_path, bucket)\n",
    "    test_folder_path = os.path.join(local_storage_path, \"test\")\n",
    "    os.makedirs(test_folder_path)\n",
    "    model_path = os.path.join(test_folder_path, 'MLPipelineModelNOPrice'+str(i)+'.pkl')\n",
    "    \n",
    "    logger.info(\"****** PICKLE FILE PATH *******\")\n",
    "    logger.info(model_path)\n",
    "    joblib.dump(model, open(model_path, 'wb'))\n",
    "    logger.info(\"folder contents: {}\".format(os.listdir(test_folder_path)))\n",
    "\n",
    "    #Storage push\n",
    "    logger.debug('********Before storage_push*******')\n",
    "    logger.debug(os.listdir(local_storage_path))\n",
    "    value = storage_utils.storage_push(bucket, local_storage_path, overwrite=True)\n",
    "\n",
    "    if value:\n",
    "        logger.debug(\"storage_push successful\")\n",
    "    else:\n",
    "        logger.debug(\"storage_push failed\")\n",
    "        shutil.rmtree(test_folder_path)\n",
    "\n",
    "\n",
    "    #Storage pull\n",
    "    logger.debug('********Before storage_pull*******')\n",
    "    logger.debug(os.listdir(local_storage_path))\n",
    "    value = storage_utils.storage_pull(bucket, local_storage_path, overwrite=True)\n",
    "\n",
    "\n",
    "    if value:\n",
    "        logger.debug(\"storage_pull successful\")\n",
    "    else:\n",
    "        logger.debug(\"storage_pull failed\")\n",
    "\n",
    "    logger.debug('********After storage_pull*******')\n",
    "    logger.info(os.listdir(test_folder_path))\n",
    "\n",
    "\n",
    "    loaded_model = joblib.load(open(model_path, 'rb'))\n",
    "    result = loaded_model.score(X,y)\n",
    "    logger.info(\"********** Result from Stored Pickle File**********\")\n",
    "    logger.info(result)\n",
    "    logger.info(\"****** PICKLE FILE NUMBER  **********\" +  str(i))\n",
    "\n",
    "\n",
    "\n",
    "####################### WITH PRICE MODEL #######################\n",
    "\n",
    "X1 = X[[\"Supplier.[Supplier]\", \n",
    "        \"cos_month\",\n",
    "        \"Cluster\",\n",
    "        \"Activity2.[Activity2]\",\n",
    "        \"Location.[Location]\",\n",
    "        'Price',\n",
    "        'Quantity',\n",
    "        'date_diff_thanksgiving',\n",
    "        'date_diff_easter',\n",
    "        'date_diff_usIDay',\n",
    "        'date_diff_christmasDay']]\n",
    "\n",
    "y = X[['LeadTime']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X1, y, random_state=42)\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "       ('imputer', SimpleImputer(strategy='mean'))\n",
    "     ])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "       ('imputer', SimpleImputer(strategy='constant'))\n",
    "      ,('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "numeric_features = ['cos_month','Cluster','Price','Quantity','date_diff_thanksgiving','date_diff_easter',\n",
    "        'date_diff_usIDay',\n",
    "        'date_diff_christmasDay']\n",
    "\n",
    "categorical_features = [\"Supplier.[Supplier]\",\"Activity2.[Activity2]\",\"Location.[Location]\"]\n",
    "\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "   transformers=[\n",
    "    ('numeric', numeric_transformer, numeric_features)\n",
    "   ,('categorical', categorical_transformer, categorical_features)\n",
    "]) \n",
    "\n",
    "\n",
    "\n",
    "percentileList = [50,75,95]\n",
    "\n",
    "### For ML Model #####\n",
    "\n",
    "for i in percentileList:\n",
    "    logger.info(\"***** Training for Percentile *******\")\n",
    "    logger.info(i)\n",
    "    model = Pipeline([('preprocessor', preprocessor),('classifier', GradientBoostingRegressor(loss=\"quantile\",\n",
    "                                        n_estimators=500, \n",
    "                                        learning_rate=0.1,\n",
    "                                        max_depth=5, \n",
    "                                        random_state=0,\n",
    "                                        alpha=(i/100)))]) \n",
    "\n",
    "    model.fit(X1,y)\n",
    "\n",
    "    bucket = \"MLPipelineModelPrice\" + str(i)\n",
    "    local_storage_path = os.path.join(user_storage_path, bucket)\n",
    "    test_folder_path = os.path.join(local_storage_path, \"test\")\n",
    "    os.makedirs(test_folder_path)\n",
    "    model_path = os.path.join(test_folder_path, 'MLPipelineModelPrice'+str(i)+'.pkl')\n",
    "    \n",
    "    logger.info(\"****** PICKLE FILE PATH *******\")\n",
    "    logger.info(model_path)\n",
    "    joblib.dump(model, open(model_path, 'wb'))\n",
    "    logger.info(\"folder contents: {}\".format(os.listdir(test_folder_path)))\n",
    "\n",
    "    #Storage push\n",
    "    logger.debug('********Before storage_push*******')\n",
    "    logger.debug(os.listdir(local_storage_path))\n",
    "    value = storage_utils.storage_push(bucket, local_storage_path, overwrite=True)\n",
    "\n",
    "    if value:\n",
    "        logger.debug(\"storage_push successful\")\n",
    "    else:\n",
    "        logger.debug(\"storage_push failed\")\n",
    "        shutil.rmtree(test_folder_path)\n",
    "\n",
    "\n",
    "    #Storage pull\n",
    "    logger.debug('********Before storage_pull*******')\n",
    "    logger.debug(os.listdir(local_storage_path))\n",
    "    value = storage_utils.storage_pull(bucket, local_storage_path, overwrite=True)\n",
    "\n",
    "\n",
    "    if value:\n",
    "        logger.debug(\"storage_pull successful\")\n",
    "    else:\n",
    "        logger.debug(\"storage_pull failed\")\n",
    "\n",
    "    logger.debug('********After storage_pull*******')\n",
    "    logger.info(os.listdir(test_folder_path))\n",
    "\n",
    "\n",
    "    loaded_model = joblib.load(open(model_path, 'rb'))\n",
    "    result = loaded_model.score(X,y)\n",
    "    logger.info(\"********** Result from Stored Pickle File**********\")\n",
    "    logger.info(result)\n",
    "    logger.info(\"****** PICKLE FILE NUMBER  **********\" +  str(i))\n",
    "\n",
    "modelpercentileList = [50]\n",
    "\n",
    "for i in modelpercentileList :\n",
    "    logger.info(\"***** Training for Percentile *******\")\n",
    "    logger.info(i)\n",
    "    model = Pipeline([('preprocessor', preprocessor),('classifier', GradientBoostingRegressor(loss=\"quantile\",\n",
    "                                        n_estimators=500, \n",
    "                                        learning_rate=0.1,\n",
    "                                        max_depth=5, \n",
    "                                        random_state=0,\n",
    "                                        alpha=(i/100)))]) \n",
    "\n",
    "    model.fit(X1,y)\n",
    "\n",
    "new_df = pd.DataFrame()\n",
    "\n",
    "result = permutation_importance(model, X1, y, n_repeats=10,random_state=42, n_jobs=2)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "dfImportance = pd.DataFrame(data=result.importances[sorted_idx].T,columns=X1.columns[sorted_idx])\n",
    "df1 = pd.DataFrame(data=dfImportance.mean()).reset_index()\n",
    "\n",
    "df1['imp'] = df1[0]\n",
    "\n",
    "df1['imp1'] = df1['imp']/(df1['imp'].sum())\n",
    "\n",
    "featureImportanceTable = df1.copy()\n",
    "\n",
    "featureImportanceTable[\"feature_new\"] = featureImportanceTable[\"index\"].apply(lambda x: x.split(\"_\")[0])\n",
    "new_df = featureImportanceTable.groupby(\"feature_new\").agg({\"imp1\":\"sum\"}).reset_index()\n",
    "featureImportanceTable = new_df.sort_values(\"imp1\", ascending=False)\n",
    "featureImportanceTable = featureImportanceTable.reset_index()\n",
    "\n",
    "featureImportanceTable = featureImportanceTable.replace(to_replace =\"date\",value =\"Holidays\")\n",
    "\n",
    "featureImportanceTable = featureImportanceTable.drop(['index'],axis=1)\n",
    "\n",
    "new_df = featureImportanceTable.copy()\n",
    "\n",
    "new_df[\"Version.[Version Name]\"] = versionName\n",
    "#new_df = new_df.drop([\"index\"],axis=1)\n",
    "\n",
    "\n",
    "new_df['Predictor.[Predictor]'] = np.arange(new_df.shape[0])\n",
    "new_df['Predictor.[Predictor]'] = new_df['Predictor.[Predictor]'].apply(lambda x: \"P\" + str(x+1))\n",
    "new_df = new_df.rename(columns={\"feature_new\":\"PredictorName\",\"imp1\":\"FeatureImportance\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bb84f5-5c63-4a75-ad9c-303afabc39ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"==============DATAFRAME==========\")\n",
    "new_df[\"[AIModel].[AIModel]\"] = \"Individual PO Lead Time Prediction\"\n",
    "new_df = new_df.replace({\"Cluster\": \"Item Cluster\",\"Supplier.[Supplier]\":\"Supplier\",\"Location.[Location]\":\"Location\",\"Activity2.[Activity2]\":\"TransMode\",\"cos_month\":\"Month\"})\n",
    "\n",
    "logger.info(new_df.columns)\n",
    "\n",
    "new_df = new_df[[\"Version.[Version Name]\",\"[AIModel].[AIModel]\",\"Predictor.[Predictor]\",\"PredictorName\",\"FeatureImportance\"]]\n",
    "logger.info(new_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[ComcastXMDataSciDev] Tenant Conda Environment",
   "language": "python",
   "name": "preprod_comcastxmdatascidev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notebook_dict": {
   "ClassName": "o9.GraphCube.Plugins.Python.PythonScript",
   "InstanceName": "AnalyticsPO004LeadTimePredictionPickle",
   "SliceKeys": [],
   "file_path": "loaded_notebooks/AnalyticsPO004LeadTimePredictionPickle.ipynb",
   "o9_selected_plugin_id": 14835
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
